{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import json\n",
    "import os\n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "data_csv = \"{}/driving_log.csv\".format(data_dir)\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "image_dim = (80,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Images and pre process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_image(image_path,dim):\n",
    "    img = cv2.imread(data_dir+\"/\"+image_path.strip())\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, dim)\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)[:,:,0]\n",
    "    norm_image = gray_image/255.\n",
    "    flatten_image = norm_image.flatten().tolist()\n",
    "    \n",
    "    return norm_image.astype(np.float32)\n",
    "\n",
    "def prepare_features(data_csv,dim,samples):\n",
    "    features = ()\n",
    "    labels = ()\n",
    "    \n",
    "    data = pd.read_csv(data_csv)\n",
    "    sample_df = data.ix[:samples-1,:]\n",
    "    sample_df['center'] = sample_df['center'].apply(lambda x :read_image(x,dim))\n",
    "    sample_df['left'] = sample_df['left'].apply(lambda x :read_image(x,dim))\n",
    "    sample_df['right'] = sample_df['right'].apply(lambda x :read_image(x,dim))\n",
    "    \n",
    "    for i,r in sample_df.iterrows():\n",
    "        center = r['center']\n",
    "        left = r['left']\n",
    "        right = r['right']\n",
    "\n",
    "        features+= (center,left,right)\n",
    "        labels+= (r['steering'],r['steering'],r['steering'])\n",
    "    \n",
    "    assert (sample_df.shape[0]*3 ==len(features)), \"Dimensions didn't match\"\n",
    "    assert (sample_df.shape[0]*3 ==len(labels)), \"Dimensions didn't match\"\n",
    "    features = np.array(features).reshape(len(features), dim[0], dim[1], 1)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    input_shape = features.shape[1:]\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    print('Image Shape {0}x{1}x{2}'.format(input_shape[0],input_shape[1],input_shape[2]))\n",
    "    \n",
    "    return (features,labels,input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 80, 40, 1)\n",
      "(300,)\n",
      "Image Shape 80x40x1\n"
     ]
    }
   ],
   "source": [
    "features,labels,input_shape = prepare_features(data_csv,image_dim,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(dropout_rate=0.0):\n",
    "    \n",
    "    nb_filters1 = 16\n",
    "    nb_filters2 = 8\n",
    "    nb_filters3 = 4\n",
    "    nb_filters4 = 2\n",
    "\n",
    "    pool_size = (2, 2)\n",
    "    kernel_size = (3, 3)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters1, kernel_size[0], kernel_size[1],border_mode='valid',input_shape=input_shape))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(nb_filters2, kernel_size[0], kernel_size[1]))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(nb_filters3, kernel_size[0], kernel_size[1]))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(nb_filters4, kernel_size[0], kernel_size[1]))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error',optimizer=Adam(),metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, features, labels):\n",
    "    # Test, Train, Valid Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.10,random_state=832289)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=0.25,random_state=832289)\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train,batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_valid, y_valid))\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Test Accuracy : %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Dropout rate using GridCV Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, nb_epoch=10, batch_size=5, verbose=1)\n",
    "dropout_rate = [0.0, 0.2, 0.5]\n",
    "param_grid = dict(dropout_rate=dropout_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_result = grid.fit(features, labels)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model using the dropouts , discovered by GridCV Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_params = grid_result.best_params_\n",
    "dropout_keep_prob = best_params['dropout_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 78, 38, 16)    160         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 78, 38, 16)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 76, 36, 8)     1160        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 76, 36, 8)     0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 74, 34, 4)     292         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 74, 34, 4)     0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 72, 32, 2)     74          activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 72, 32, 2)     0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 36, 16, 2)     0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 36, 16, 2)     0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1152)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 16)            18448       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 16)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 16)            272         activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 16)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 16)            272         activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 16)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 16)            0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             17          dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 20,695\n",
      "Trainable params: 20,695\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 202 samples, validate on 68 samples\n",
      "Epoch 1/150\n",
      "202/202 [==============================] - 3s - loss: 0.0127 - acc: 0.7228 - val_loss: 0.0099 - val_acc: 0.7206\n",
      "Epoch 2/150\n",
      "202/202 [==============================] - 0s - loss: 0.0123 - acc: 0.7228 - val_loss: 0.0091 - val_acc: 0.7206\n",
      "Epoch 3/150\n",
      "202/202 [==============================] - 0s - loss: 0.0122 - acc: 0.7228 - val_loss: 0.0090 - val_acc: 0.7206\n",
      "Epoch 4/150\n",
      "202/202 [==============================] - 0s - loss: 0.0122 - acc: 0.7228 - val_loss: 0.0090 - val_acc: 0.7206\n",
      "Epoch 5/150\n",
      "202/202 [==============================] - 0s - loss: 0.0118 - acc: 0.7228 - val_loss: 0.0090 - val_acc: 0.7206\n",
      "Epoch 6/150\n",
      "202/202 [==============================] - 0s - loss: 0.0114 - acc: 0.7228 - val_loss: 0.0087 - val_acc: 0.7206\n",
      "Epoch 7/150\n",
      "202/202 [==============================] - 0s - loss: 0.0113 - acc: 0.7228 - val_loss: 0.0085 - val_acc: 0.7206\n",
      "Epoch 8/150\n",
      "202/202 [==============================] - 0s - loss: 0.0111 - acc: 0.7228 - val_loss: 0.0084 - val_acc: 0.7206\n",
      "Epoch 9/150\n",
      "202/202 [==============================] - 0s - loss: 0.0106 - acc: 0.7228 - val_loss: 0.0088 - val_acc: 0.7206\n",
      "Epoch 10/150\n",
      "202/202 [==============================] - 0s - loss: 0.0104 - acc: 0.7228 - val_loss: 0.0081 - val_acc: 0.7206\n",
      "Epoch 11/150\n",
      "202/202 [==============================] - 0s - loss: 0.0106 - acc: 0.7228 - val_loss: 0.0078 - val_acc: 0.7206\n",
      "Epoch 12/150\n",
      "202/202 [==============================] - 0s - loss: 0.0096 - acc: 0.7228 - val_loss: 0.0082 - val_acc: 0.7206\n",
      "Epoch 13/150\n",
      "202/202 [==============================] - 0s - loss: 0.0091 - acc: 0.7228 - val_loss: 0.0072 - val_acc: 0.7206\n",
      "Epoch 14/150\n",
      "202/202 [==============================] - 0s - loss: 0.0087 - acc: 0.7228 - val_loss: 0.0080 - val_acc: 0.7206\n",
      "Epoch 15/150\n",
      "202/202 [==============================] - 0s - loss: 0.0082 - acc: 0.7228 - val_loss: 0.0064 - val_acc: 0.7206\n",
      "Epoch 16/150\n",
      "202/202 [==============================] - 0s - loss: 0.0073 - acc: 0.7228 - val_loss: 0.0074 - val_acc: 0.7206\n",
      "Epoch 17/150\n",
      "202/202 [==============================] - 0s - loss: 0.0074 - acc: 0.7228 - val_loss: 0.0056 - val_acc: 0.7206\n",
      "Epoch 18/150\n",
      "202/202 [==============================] - 0s - loss: 0.0065 - acc: 0.7228 - val_loss: 0.0054 - val_acc: 0.7206\n",
      "Epoch 19/150\n",
      "202/202 [==============================] - 0s - loss: 0.0053 - acc: 0.7228 - val_loss: 0.0050 - val_acc: 0.7206\n",
      "Epoch 20/150\n",
      "202/202 [==============================] - 0s - loss: 0.0041 - acc: 0.7228 - val_loss: 0.0042 - val_acc: 0.7206\n",
      "Epoch 21/150\n",
      "202/202 [==============================] - 0s - loss: 0.0034 - acc: 0.7228 - val_loss: 0.0036 - val_acc: 0.7206\n",
      "Epoch 22/150\n",
      "202/202 [==============================] - 0s - loss: 0.0028 - acc: 0.7228 - val_loss: 0.0038 - val_acc: 0.7206\n",
      "Epoch 23/150\n",
      "202/202 [==============================] - 0s - loss: 0.0025 - acc: 0.7228 - val_loss: 0.0036 - val_acc: 0.7206\n",
      "Epoch 24/150\n",
      "202/202 [==============================] - 0s - loss: 0.0025 - acc: 0.7228 - val_loss: 0.0033 - val_acc: 0.7206\n",
      "Epoch 25/150\n",
      "202/202 [==============================] - 0s - loss: 0.0018 - acc: 0.7228 - val_loss: 0.0026 - val_acc: 0.7206\n",
      "Epoch 26/150\n",
      "202/202 [==============================] - 0s - loss: 0.0012 - acc: 0.7228 - val_loss: 0.0027 - val_acc: 0.7206\n",
      "Epoch 27/150\n",
      "202/202 [==============================] - 0s - loss: 0.0011 - acc: 0.7228 - val_loss: 0.0024 - val_acc: 0.7206\n",
      "Epoch 28/150\n",
      "202/202 [==============================] - 0s - loss: 8.1865e-04 - acc: 0.7228 - val_loss: 0.0021 - val_acc: 0.7206\n",
      "Epoch 29/150\n",
      "202/202 [==============================] - 0s - loss: 6.7575e-04 - acc: 0.7228 - val_loss: 0.0019 - val_acc: 0.7206\n",
      "Epoch 30/150\n",
      "202/202 [==============================] - 0s - loss: 6.1704e-04 - acc: 0.7228 - val_loss: 0.0019 - val_acc: 0.7206\n",
      "Epoch 31/150\n",
      "202/202 [==============================] - 0s - loss: 5.5846e-04 - acc: 0.7228 - val_loss: 0.0018 - val_acc: 0.7206\n",
      "Epoch 32/150\n",
      "202/202 [==============================] - 0s - loss: 5.4945e-04 - acc: 0.7228 - val_loss: 0.0019 - val_acc: 0.7206\n",
      "Epoch 33/150\n",
      "202/202 [==============================] - 0s - loss: 5.4993e-04 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 34/150\n",
      "202/202 [==============================] - 0s - loss: 3.7249e-04 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 35/150\n",
      "202/202 [==============================] - 0s - loss: 3.6881e-04 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 36/150\n",
      "202/202 [==============================] - 0s - loss: 3.0892e-04 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 37/150\n",
      "202/202 [==============================] - 0s - loss: 2.8998e-04 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 38/150\n",
      "202/202 [==============================] - 0s - loss: 2.8791e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 39/150\n",
      "202/202 [==============================] - 0s - loss: 2.3995e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 40/150\n",
      "202/202 [==============================] - 0s - loss: 2.2974e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 41/150\n",
      "202/202 [==============================] - 0s - loss: 2.1923e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 42/150\n",
      "202/202 [==============================] - 0s - loss: 2.1675e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 43/150\n",
      "202/202 [==============================] - 0s - loss: 1.6813e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 44/150\n",
      "202/202 [==============================] - 0s - loss: 1.6779e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 45/150\n",
      "202/202 [==============================] - 0s - loss: 1.3453e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 46/150\n",
      "202/202 [==============================] - 0s - loss: 1.8446e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 47/150\n",
      "202/202 [==============================] - 0s - loss: 1.3285e-04 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 48/150\n",
      "202/202 [==============================] - 0s - loss: 1.3374e-04 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 49/150\n",
      "202/202 [==============================] - 0s - loss: 1.3232e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 50/150\n",
      "202/202 [==============================] - 0s - loss: 1.0123e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 51/150\n",
      "202/202 [==============================] - 0s - loss: 9.4440e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 52/150\n",
      "202/202 [==============================] - 0s - loss: 9.5616e-05 - acc: 0.7228 - val_loss: 0.0017 - val_acc: 0.7206\n",
      "Epoch 53/150\n",
      "202/202 [==============================] - 0s - loss: 1.1245e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 54/150\n",
      "202/202 [==============================] - 0s - loss: 1.1450e-04 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 55/150\n",
      "202/202 [==============================] - 0s - loss: 9.4285e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 56/150\n",
      "202/202 [==============================] - 0s - loss: 9.6975e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 57/150\n",
      "202/202 [==============================] - 0s - loss: 8.2167e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 58/150\n",
      "202/202 [==============================] - 0s - loss: 8.5726e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 59/150\n",
      "202/202 [==============================] - 0s - loss: 7.0364e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 60/150\n",
      "202/202 [==============================] - 0s - loss: 6.6286e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 61/150\n",
      "202/202 [==============================] - 0s - loss: 7.7741e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 62/150\n",
      "202/202 [==============================] - 0s - loss: 6.6669e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 63/150\n",
      "202/202 [==============================] - 0s - loss: 6.5226e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 64/150\n",
      "202/202 [==============================] - 0s - loss: 6.3887e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 65/150\n",
      "202/202 [==============================] - 0s - loss: 6.8211e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 66/150\n",
      "202/202 [==============================] - 0s - loss: 6.1533e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 67/150\n",
      "202/202 [==============================] - 0s - loss: 6.5834e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 68/150\n",
      "202/202 [==============================] - 0s - loss: 5.4437e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 69/150\n",
      "202/202 [==============================] - 0s - loss: 5.3831e-05 - acc: 0.7228 - val_loss: 0.0015 - val_acc: 0.7206\n",
      "Epoch 70/150\n",
      "202/202 [==============================] - 0s - loss: 5.2094e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 71/150\n",
      "202/202 [==============================] - 0s - loss: 4.1433e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 72/150\n",
      "202/202 [==============================] - 0s - loss: 3.6196e-05 - acc: 0.7228 - val_loss: 0.0015 - val_acc: 0.7206\n",
      "Epoch 73/150\n",
      "202/202 [==============================] - 0s - loss: 4.2123e-05 - acc: 0.7228 - val_loss: 0.0015 - val_acc: 0.7206\n",
      "Epoch 74/150\n",
      "202/202 [==============================] - 0s - loss: 3.0642e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 75/150\n",
      "202/202 [==============================] - 0s - loss: 3.4729e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 76/150\n",
      "202/202 [==============================] - 0s - loss: 3.2410e-05 - acc: 0.7228 - val_loss: 0.0015 - val_acc: 0.7206\n",
      "Epoch 77/150\n",
      "202/202 [==============================] - 0s - loss: 3.1581e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 78/150\n",
      "202/202 [==============================] - 0s - loss: 3.0072e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 79/150\n",
      "202/202 [==============================] - 0s - loss: 2.8814e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 80/150\n",
      "202/202 [==============================] - 0s - loss: 2.5385e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 81/150\n",
      "202/202 [==============================] - 0s - loss: 2.3112e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 82/150\n",
      "202/202 [==============================] - 0s - loss: 2.1553e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 83/150\n",
      "202/202 [==============================] - 0s - loss: 2.0997e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 84/150\n",
      "202/202 [==============================] - 0s - loss: 2.0508e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 85/150\n",
      "202/202 [==============================] - 0s - loss: 1.8885e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 86/150\n",
      "202/202 [==============================] - 0s - loss: 1.7899e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 87/150\n",
      "202/202 [==============================] - 0s - loss: 1.7077e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 88/150\n",
      "202/202 [==============================] - 0s - loss: 1.6315e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 89/150\n",
      "202/202 [==============================] - 0s - loss: 1.6266e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 90/150\n",
      "202/202 [==============================] - 0s - loss: 1.6508e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 91/150\n",
      "202/202 [==============================] - 0s - loss: 1.7060e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 92/150\n",
      "202/202 [==============================] - 0s - loss: 1.7170e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 93/150\n",
      "202/202 [==============================] - 0s - loss: 1.6293e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 94/150\n",
      "202/202 [==============================] - 0s - loss: 1.5407e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 95/150\n",
      "202/202 [==============================] - 0s - loss: 1.5798e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 96/150\n",
      "202/202 [==============================] - 0s - loss: 1.3428e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 97/150\n",
      "202/202 [==============================] - 0s - loss: 1.6929e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 98/150\n",
      "202/202 [==============================] - 0s - loss: 1.4686e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 99/150\n",
      "202/202 [==============================] - 0s - loss: 1.5015e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 100/150\n",
      "202/202 [==============================] - 0s - loss: 1.2006e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 101/150\n",
      "202/202 [==============================] - 0s - loss: 1.2687e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 102/150\n",
      "202/202 [==============================] - 0s - loss: 1.2396e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 103/150\n",
      "202/202 [==============================] - 0s - loss: 1.1184e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 104/150\n",
      "202/202 [==============================] - 0s - loss: 1.1645e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 105/150\n",
      "202/202 [==============================] - 0s - loss: 1.2641e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 106/150\n",
      "202/202 [==============================] - 0s - loss: 1.0424e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 107/150\n",
      "202/202 [==============================] - 0s - loss: 1.0180e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 108/150\n",
      "202/202 [==============================] - 0s - loss: 9.3037e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 109/150\n",
      "202/202 [==============================] - 0s - loss: 9.3670e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 110/150\n",
      "202/202 [==============================] - 0s - loss: 1.0090e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 111/150\n",
      "202/202 [==============================] - 0s - loss: 1.1691e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 112/150\n",
      "202/202 [==============================] - 0s - loss: 1.2725e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 113/150\n",
      "202/202 [==============================] - 0s - loss: 1.1655e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 114/150\n",
      "202/202 [==============================] - 0s - loss: 8.9125e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 115/150\n",
      "202/202 [==============================] - 0s - loss: 1.0876e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 116/150\n",
      "202/202 [==============================] - 0s - loss: 1.0852e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 117/150\n",
      "202/202 [==============================] - 0s - loss: 9.9298e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 118/150\n",
      "202/202 [==============================] - 0s - loss: 8.4600e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 119/150\n",
      "202/202 [==============================] - 0s - loss: 7.2463e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 120/150\n",
      "202/202 [==============================] - 0s - loss: 7.5281e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 121/150\n",
      "202/202 [==============================] - 0s - loss: 8.9247e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 122/150\n",
      "202/202 [==============================] - 0s - loss: 9.8751e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 123/150\n",
      "202/202 [==============================] - 0s - loss: 8.2380e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 124/150\n",
      "202/202 [==============================] - 0s - loss: 7.7791e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 125/150\n",
      "202/202 [==============================] - 0s - loss: 7.6023e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 126/150\n",
      "202/202 [==============================] - 0s - loss: 7.9733e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 127/150\n",
      "202/202 [==============================] - 0s - loss: 6.6084e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 128/150\n",
      "202/202 [==============================] - 0s - loss: 8.0374e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 129/150\n",
      "202/202 [==============================] - 0s - loss: 1.1636e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 130/150\n",
      "202/202 [==============================] - 0s - loss: 2.0858e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 131/150\n",
      "202/202 [==============================] - 0s - loss: 1.7040e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 132/150\n",
      "202/202 [==============================] - 0s - loss: 1.2411e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 133/150\n",
      "202/202 [==============================] - 0s - loss: 1.1396e-05 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 134/150\n",
      "202/202 [==============================] - 0s - loss: 7.7216e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 135/150\n",
      "202/202 [==============================] - 0s - loss: 7.2211e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 136/150\n",
      "202/202 [==============================] - 0s - loss: 6.0413e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 137/150\n",
      "202/202 [==============================] - 0s - loss: 7.4490e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 138/150\n",
      "202/202 [==============================] - 0s - loss: 6.7658e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 139/150\n",
      "202/202 [==============================] - 0s - loss: 5.4827e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 140/150\n",
      "202/202 [==============================] - 0s - loss: 4.9564e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 141/150\n",
      "202/202 [==============================] - 0s - loss: 6.2248e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 142/150\n",
      "202/202 [==============================] - 0s - loss: 5.0064e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 143/150\n",
      "202/202 [==============================] - 0s - loss: 5.0607e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 144/150\n",
      "202/202 [==============================] - 0s - loss: 5.2335e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 145/150\n",
      "202/202 [==============================] - 0s - loss: 4.4941e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 146/150\n",
      "202/202 [==============================] - 0s - loss: 4.4184e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 147/150\n",
      "202/202 [==============================] - 0s - loss: 4.2086e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 148/150\n",
      "202/202 [==============================] - 0s - loss: 4.7477e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 149/150\n",
      "202/202 [==============================] - 0s - loss: 5.7901e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Epoch 150/150\n",
      "202/202 [==============================] - 0s - loss: 5.2636e-06 - acc: 0.7228 - val_loss: 0.0016 - val_acc: 0.7206\n",
      "Test Accuracy : acc: 90.00%\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "batch_size = 64\n",
    "nb_epoch = 150\n",
    "model = create_model(0.0)\n",
    "model.summary()\n",
    "train_and_evaluate_model(model,features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_file='model.json'\n",
    "model_weights='model.h5'\n",
    "\n",
    "def save_model(model_file,model_weights):\n",
    "    json_string = model.to_json()\n",
    "    with open(model_file, 'w') as outfile:\n",
    "        json.dump(json_string, outfile)\n",
    "        model.save_weights(model_weights)\n",
    "        print(\"Completed... Model Saved\")\n",
    "\n",
    "if model_file in os.listdir():\n",
    "    print(\"The file already exists\")\n",
    "    print(\"Want to overwite? y or n\")\n",
    "    is_overwrite = input()\n",
    "    if is_overwrite.lower() == \"y\":\n",
    "        save_model(model_file,model_weights)\n",
    "    else:\n",
    "        print(\"the model is not saved\")\n",
    "else:\n",
    "    save_model(model_file,model_weights)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
